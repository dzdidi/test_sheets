\chapter{OOP and FP}
\section{Object Oriented Programming}
"Programming languages with objects and classes typically provide dynamic lookup, abstraction, subtyping, and inheritance. These are the four main language concepts for object-oriented programming. They may be summarized in the following manner:
\textit{Dynamic lookup} means that when a message is sent to an object, the function code (or method) to be executed is determined by the way that the object is implemented, not some static property of the pointer or variable used to name the object. In other words, the object “chooses” how to respond to a message, and different objects may respond to the same message in different ways.
\textit{Abstraction} means that implementation details are hidden inside a program unit with a specific interface. For objects, the interface usually consists of a set of public functions (or public methods) that manipulate hidden data.
\textit{Subtyping} means that if some object a has all of the functionality of another object b, then we may use a in any context expecting b.
\textit{Inheritance} is the ability to reuse the definition of one kind of object to define another kind of object.
"\cite{concepts}

\subsection{Design Principles}

Agile design is a process, not an event.
It's the continous application of principles, patterns, and practices to improve the structure and readability of the software. 
It is the dedication to keep the design of the system as simple, clean, and expressive as possible at all times.\cite{MartinASD}

Symptoms of not agile design \cite{MartinASD}:
\begin{itemize}
	\item Rigidity - The design is difficult to change;
	\item Fragility - The design is easy to break;
	\item Immobility - The design is difficult to reuse;
	\item Viscosity - It is difficult to do the right thing;
	\item Needless complexity - Overdesign;
	\item Needless repetition - Mouse abuse;
	\item Opacity - Disorganized expression;
\end{itemize}
%\paragraph{Rigidity (The design is difficult to change)} - Rigidity is the tendency for software to be difficult to change, even in simple ways. A design is rigid if a single change causes a cascade of subsequent changes in dependent modules. The more modules that must be changed, the more rigid the design. Most developers have faced this situation in one way or another.  They are asked to make what appears to be a simple change. They look the change over and make a reasonable estimate of the work required. But later, as they work through the change, they find that there are unanticipated repercussions to the change. The developers find themselves chasing the change through huge portions of the code, modifying far more modules than they had first estimated, and discovering thread after thread of other changes that they must remember to make.In the end, the changes take far longer than the initial estimate.When asked why their estimate was so poor, they repeat the traditional software developers lament: "It was a lot more complicated than I thought!"

%\paragraph{Fragility (The design is easy to break)} - Fragility is the tendency of a program to break in many places when a single change is made. Often, the new problems are in areas that have no conceptual relationship with the area that was changed. Fixing those problems leads to even more problems, and the development team begins to resemble a dog chasing its tail. As the fragility of a module increases, the likelihood that a change will introduce unexpected problems approaches certainty. This seems absurd, but such modules are not at all uncommon. These are the modules that are continually in need of repair, the ones that are never off the bug list. These modules are the ones that the developers know need to be redesigned, but nobody wants to face the spectre of redesigning them. These modules are the ones that get worse the more you fix them.

%\paragraph{Immobility (The design is difficult to reuse)} - A design is immobile when it contains parts that could be useful in other systems, but the effort and risk involved with separating those parts from the original system are too great. This is an unfortunate but very common occurrence.

%\paragraph{Viscosity (It is difficult to do the right thing)} - Viscosity comes in two forms: viscosity of the software and viscosity of the environment. When faced with a change, developers usually find more than one way to make that change. Some of the ways preserve the design; others do not (i.e., they are hacks). When the design-preserving methods are more difficult to use than the hacks, the viscosity of the design is high. It is easy to do the wrong thing but difficult to do the right thing. We want to design our software such that the changes that preserve the design are easy to make. Viscosity of environment comes about when the development environment is slow and inefficient. For example, if compile times are very long, developers will be tempted to make changes that don't force large recompiles, even though those changes don't preserve the design.  If the source code control system requires hours to check in just a few files, developers will be tempted to make changes that require as few check-ins as possible, regardless of whether the design is preserved. In both cases, a viscous project is one in which the design of the software is difficult to preserve.  We want to create systems and project environments that make it easy to preserve and improve the design.


%\paragraph{Needless complexity (Overdesign)} - A design smells of needless complexity when it contains elements that aren't currently useful.This frequently happens when developers anticipate changes to the requirements and put facilities in the software to deal with those potential changes. At first, this may seem like a good thing to do. After all, preparing for future changes should keep our code flexible and prevent nightmarish changes later. Unfortunately, the effect is often just the opposite. By preparing for many contingencies, the design becomes littered with constructs that are never used. Some of those preparations may pay off, but many more do not.  Meanwhile, the design carries the weight of these unused design elements. This makes the software complex and difficult to understand.

%\paragraph{Needless repetition (Mouse abuse)} - Cut and paste may be useful text-editing operations, but they can be disastrous code-editing operations. All too often, software systems are built on dozens or hundreds of repeated code elements. It happens like this: Ralph needs to write some code that fravles the arvadent. He looks around in other parts of the code where he suspects other arvadent fravling has occurred and finds a suitable stretch of code. He cuts and pastes that code into his module and makes the suitable modifications. Unbeknownst to Ralph, the code he scraped up with his mouse was put there by Todd, who scraped it out of a module written by Lilly. Lilly was the first to fravle an arvadent, but she realized that fravling an arvadent was very similar to fravling a garnatosh.  She found some code somewhere that fravled a garnatosh, cut and paste it into her module, and modified it as necessary.When the same code appears over and over again, in slightly different forms, the developers are missing an abstraction. Finding all the repetition and eliminating it with an appropriate abstraction may not be high on their priority list, but it would go a long way toward making the system easier to understand and maintain. When there is redundant code in the system, the job of changing the system can become arduous. Bugs found in such a repeating unit have to be fixed in every repetition. However, since each repetition is slightly different from every other, the fix is not always the same.


%\paragraph{Opacity (Disorganized expression)} - Opacity is the tendency of a module to be difficult to understand. Code can be written in a clear and expressive manner, or it can be written in an opaque and convoluted manner. Code that evolves over time tends to become more and more opaque with age. A constant effort to keep the code clear and expressive is required in order to keep opacity to a minimum.When developers first write a module, the code may seem clear to them.After all, they have immersed themselves in it and understand it at an intimate level.Later, after the intimacy has worn off, they may return to that module and wonder how they could have written anything so awful. To prevent this, developers need to put themselves in their readers' shoes and make a concerted effort to refactor their code so that their readers can understand it. They also need to have their code reviewed by others.

Foundamental Object Oriented design principles\cite{Dooley}\cite{MartinASD} :
\begin{itemize}
 \item Closing - Encapsulate things in your design that are likely to change.
 \item Code to an Interface - rather then to an implementation. 
 \item Do not repeat yourself (DRY) - Avoid duplicate code.
 \item The Single-Responsibility Principle (SRP) - A class should have only one reason to change
 \item The Open/Closed Principle (OCP) - Software entities (classes, modules, functions, etc.) should be open for extension but closed for modification
 \item The Liskov Substitution Principle - Subtypes must be substitutable for their base types.
 \item The Dependency-Inversion Principle - A) High-level modules should not depend on low-level modules. 
Both should depend on abstractions. 
B) Abstractions should not depend upon details. 
Details should depend upon abstractions.
 \item The Interface Segregation Principle (ISP) - Clients should not be forced to depend on methods they do not use.
 \item Principles of Least Knowledge (PLK) - Talk to your immediate friends. 
 \item Principle of Loose Coupling - object that interact should be loosely coupled with well-defined intefaces.
\end{itemize}

%\paragraph{Closing} - Encapsulate things in your design that are likely to change. %This means to protect your classes from unnecessary change by separating the features and methods of a class that relatively constant throughout the program from that will change. By separating the two types of features, we isolate the parts that will change a lot into a separate class (or classes) that we can depend on changing, and we increase our flexibility and ease of change. \cite{Dooley}

%\paragraph{Code to an Interface} rather then to an implementation. \cite{Dooley}

%\paragraph{Do not repeat yourself (DRY)} - Avoid duplicate code. % Whenever you find common behaviour in two or more places, look to abstract tht behaviour into a class and then resue that behaviour in the common concrete classes. Satisfy one requirement in one place in your code\cite{Dooley}

%\paragraph{The Single-Responsibility Principle (SRP)} - A class should have only one reason to change.%In the context of the SRP, we define a responsibility to be a reason for change. If you can think of more than one motive for changing a class, that class has more than one responsibility. This is sometimes difficult to see. We are accustomed to thinking of responsibility in groups. The Single-Responsibility Principle is one of the simplest of the principles but one of the most difficult to get right. Conjoining responsibilities is something that we do naturally. Finding and separating those responsibilities is much of what software design is really about. Indeed, the rest of the principles we discuss come back to this issue in one way or another.\cite{MartinASD}\cite{Dooley}


%\paragraph{The Open/Closed Principle (OCP)} - Software entities (classes, modules, functions, etc.) should be open for extension but closed for modification.
%When a single change to a program results in a cascade of changes to dependent modules, the design smells of rigidity. OCP advises us to refactor the system so that further changes of that kind will not cause more modifications.  If OCP is applied well, further changes of that kind are achieved by adding new code, not by changing old code that already works.  This may seem like motherhood and apple piethe golden, unachievable idealbut in fact, there are some relatively simple and effective strategies for approaching that ideal. Modules that conform to OCP have two primary attributes. They are open for extension. This means that the behavior of the module can be extended. As the requirements of the application change, we can extend the module with new behaviors that satisfy those changes.  In other words, we are able to change what the module does. 1. They are closed for modification. Extending the behavior of a module does not result in changes to the source, or binary, code of the module. The binary executable version of the modulewhether in a linkable library, a DLL, or a .EXE fileremains untouched. 2. It would seem that these two attributes are at odds.  The normal way to extend the behavior of a module is to make changes to the source code of that module.  A module that cannot be changed is normally thought to have a fixed behavior.

%In many ways, the Open/Closed Principle is at the heart of object-oriented design.  Conformance to this principle is what yields the greatest benefits claimed for object-oriented technology: flexibility, reusability, and maintainability. Yet conformance to this principle is not achieved simply by using an object-oriented programming language. Nor is it a good idea to apply rampant abstraction to every part of the application.  Rather, it requires a dedication on the part of the developers to apply abstraction only to those parts of the program that exhibit frequent change. Resisting premature abstraction is as important as abstraction itself.\cite{MartinASD}\cite{Dooley}
%\paragraph{The Liskov Substitution Principle} - Subtypes must be substitutable for their base types.
%The Open/Closed Principle is at the heart of many of the claims made for object-oriented design. When this principle is in effect, applications are more maintainable, reusable, and robust.  The Liskov Substitution Principle is one of the prime enablers of OCP. The substitutability of subtypes allows a module, expressed in terms of a base type, to be extensible without modification. That substitutability must be something that developers can depend on implicitly. Thus, the contract of the base type has to be well and prominently understood, if not explicitly enforced, by the code. The term IS-A is too broad to act as a definition of a subtype.  The true definition of a subtype is substitutable, where substitutability is defined by either an explicit or implicit contract.\cite{MartinASD}\cite{Dooley}


%\paragraph{The Dependency-Inversion Principle} - A) High-level modules should not depend on low-level modules. 
%Both should depend on abstractions. 
%B) Abstractions should not depend upon details. 
%Details should depend upon abstractions.

%Consider the implications of high-level modules that depend on low-level modules.  It is the high-level modules that contain the important policy decisions and business models of an application. These modules contain the identity of the application.  Yet when these modules depend on the lower-level modules, changes to the lower-level modules can have direct effects on the higher-level modules and can force them to change in turn. This predicament is absurd!  It is the high-level, policy-setting modules that ought to be influencing the low-level detailed modules. The modules that contain the high-level business rules should take precedence over, and be independent of, the modules that contain the implementation details.  Highlevel modules simply should not depend on low-level modules in any way. Moreover, it is high-level, policy-setting modules that we want to be able to reuse. We are already quite good at reusing low-level modules in the form of subroutine libraries. When high-level modules depend on low-level modules, it becomes very difficult to reuse those high-level modules in different contexts.  However, when the high-level modules are independent of the low-level modules, the highlevel modules can be reused quite simply. This principle is at the very heart of framework design. Traditional procedural programming creates a dependency structure in which policy depends on detail. This is unfortunate, since the policies are then vulnerable to changes in the details.  Objectoriented programming inverts that dependency structure such that both details and policies depend on abstraction, and service interfaces are often owned by their clients. Indeed, this inversion of dependencies is the hallmark of good object-oriented design.  It doesn't matter what language a program is written in.  If its dependencies are inverted, it has an OO design. If its dependencies are not inverted, it has a procedural design. The principle of dependency inversion is the fundamental low-level mechanism behind many of the benefits claimed for object-oriented technology. Its proper application is necessary for the creation of reusable frameworks. It is also critically important for the construction of code that is resilient to change.  Since abstractions and details are isolated from each other, the code is much easier to maintain.\cite{MartinASD}\cite{Dooley}

%\paragraph{ The Interface Segregation Principle (ISP)} - Clients should not be forced to depend on methods they do not use.
%When clients are forced to depend on methods they don't use, those clients are subject to changes to those methods. This results in an inadvertent coupling between all the clients. Said another way, when a client depends on a class that contains methods that the client does not use but that other clients do use, that client will be affected by the changes that those other clients force on the class. We would like to avoid such couplings where possible, and so we want to separate the interfaces. This principle deals with the disadvantages of "fat" interfaces.  Classes whose interfaces are not cohesive have "fat" interfaces.  In other words, the interfaces of the class can be broken up into groups of methods.  Each group serves a different set of clients. Thus, some clients use one group of methods, and other clients use the other groups. ISP acknowledges that there are objects that require noncohesive interfaces; however, it suggests that clients should not know about them as a single class. Instead, clients should know about abstract base classes that have cohesive interfaces. Fat classes cause bizarre and harmful couplings between their clients.  When one client forces a change on the fat class, all the other clients are affected.  Thus, clients should have to depend only on methods that they call.  This can be achieved by breaking the interface of the fat class into many client-specific interfaces. Each client-specific interface declares only those functions that its particular client or client group invoke.  The fat class can then inherit all the client-specific interfaces and implement them.  This breaks the dependence of the clients on methods that they don't invoke and allows the clients to be independent of one another.\cite{MartinASD}\cite{Dooley}

%\paragraph{Principles of Least Knowledge (PLK)} - Talk to your immediate friends. 
%The complement to strong cohesion in an application is loose coupling. That’s what the Principle of Least Knowledge is all about. It says that classes should collaborate indirectly with as few other classes as possible.This leads us to a corollary to the PLK – keep dependencies to a minimum. This is the crux of loose coupling. By interacting with only a few other classes, you make your class more flexible and less likely to contain errors.  \cite{Dooley}

%\paragraph{Principle of Loose Coupling} - object that interact should be loosely coupled with well-defined intefaces.\cite{Dooley}

\subsection{Design Patterns}
"Design patterns make it easier to resuse successful designs and architectures. Expressing proven techniques as design patterns makes them more accessible to developers of new sysntems. Desugb patterns help to choose design alternatives that make a system reusable and avoid alternatives that compromise resuability. "\cite{DesignPatterns}


Creational Design Patterns abstract the instantiation process and provide independecne for object creation, composition and representation. 
Factory - "The Factory Design Pattern is probably the most used design pattern in modern programming languages like Java and C\#. It comes in different variants and implementations. If you are searching for it, most likely, you'll find references about the GoF patterns: Factory Method and Abstract Factory.
\begin{itemize}
\item creates objects without exposing the instantiation logic to the client.
\item refers to the newly created object through a common interface
\end{itemize}"\cite{oosite}\\


Abstract Factroy(for HO test sheets) - "Using this pattern a framework is defined, which produces objects that follow a general pattern and at runtime this factory is paired with any concrete factory to produce objects that follow the pattern of a certain country. In other words, the Abstract Factory is a super-factory which creates other factories (Factory of factories).
Abstract Factory offers the interface for creating a family of related objects, without explicitly specifying their classes."\cite{oosite}\\

ObjectPool (passes sheet object through streams) - "pattern offer a mechanism to reuse objects that are expensive to create. .
"\cite{oosite}\\

Behavior Patterns - bahaviour patters are concerned with algorithms and the assignmens of resposnibilities between objects. Beahvioral pattersndescribe not just patterns of objects or classes but also the patterns of communication between them, These patterns characterize complex control flow thats difficult to tfollow at run time. They shift your focus away from flow of control to let you concentrate just on the way objects are interconnected"\cite{DesignPatterns}
Interpreter (translation from ts to js) - "The Interpreter is one of the Design Patterns published in the GoF which is not really used. Ussualy the Interpreter Pattern is described in terms of formal grammars, like it was described in the original form in the GoF but the area where this design pattern can be applied can be extended.
\begin{itemize}
\item Given a language, define a representation for its grammar along with an interpreter that uses the representation to interpret sentences in the language.
\item Map a domain to a language, the language to a grammar, and the grammar to a hierarchical object-oriented design
\end{itemize}"\cite{oosite}\\
Strategy (multipiping/piping of streams) - "lets the algorithm vary independently from clients that use it."\cite{oosite}\\
Observer (event emitor) - "The Observer Design Pattern can be used whenever a subject has to be observed by one or more observers."\cite{oosite}
Visitor (callbacks in JS) - "\begin{itemize}
\item Represents an operation to be performed on the elements of an object structure.
\item Visitor lets you define a new operation without changing the classes of the elements on which it operates.
\end{itemize}
"\cite{oosite}\\


\section{Functional Programming}
Functional Programming languages are languages which supports functions as a first-class citizens. Which mean that languge provides an opotunity to store them in data strucutures, pass and return them from functions they are higher-order functions\cite{Hudak}.

Declarative languages in contrast to imperative ones  are characterized as having no implicit state. Functional languages are declarative languages whose underlaying computational model is the function.\cite{Hudak}

The most fundamental influence developing of functional languages was the work of Alnso Church on lambda calculus. "Church’s lambda calculus was the first suitable treatment of the computational aspects of functions."\cite{Hudak}\\

Introduction of lambda functions to Java SE 8 as a new and important feature\cite{javase} indicates the growing need of imperative programming benefits in an Enterprise Software development.\\

"Modeling with objects is powerful and intuitive, largely because this matches the perception of interacting with a world of which we are part. However, as we've seen repeatedly throughout this chapter, these models raise thorny problems of constraining the order of events and of synchronizing multiple processes. The possibility of avoiding these problems has stimulated the development of functional programming languages, which do not include any provision for assignment or mutable data. In such a language, all procedures implement well-defined mathematical functions of their arguments, whose behavior does not change. The functional approach is extremely attractive for dealing with concurrent systems." \cite{sicp}\\

"John Backus, the inventor of Fortran, gave high visibility to functional programming when he was awarded the ACM Turing award in 1978. His acceptance speech (Backus 1978) strongly advocated the functional approach. A good overview of functional programming is given in Henderson 1980 and in Darlington, Henderson, and Turner 1982."\cite{sicp}\\

\section{Streams}
"A stream is an abstract interface implemented by various objects in Node.js." \cite{nodejsstreams}
"Dominic Tarr (one of top contributors to the Node.js community \cite{nodejscontributors}), defines streams as node's best and most misunderstood idea."\cite{nodejsbook}

Streams are the classic example of Pipe-and-filter architecture.

"In an event-based platform such as Node.js, the most efficient way to handle I/O is in real time, consuming the input as soon as it is available and sending the output as soon as it is produced by the application."\cite{nodejsbook}

\begin{itemize}
	\item Spatial efficiency
	\item Time efficiency
	\item Composability
\end{itemize}


%\paragraph{Spatial efficiency.}
%"First of all, streams allow us to do things that would not be possible, by buffering data and processing it all at once. For example, consider the case in which we have to read a very big file, let's say, in the order of hundreds of megabytes or even gigabytes. Clearly, using an API that returns a big buffer when the file is completely read, is not a good idea.Imagine reading a few of these big files concurrently; our application will easily run out of memory. Besides that, buffers in V8 (default NodeJS engine) cannot be bigger than 0x3FFFFFFF bytes (a little bit less than 1 GB). So, we might hit a wall way before running out of physical memory." \cite{nodejsbook}

%\paragraph{Time efficiency.}"Let's now consider the case of an application that compresses a file and uploads it to a remote HTTP server, which in turn decompresses and saves it on the filesystem.If our client was implemented using a buffered API, the upload would start only when the entire file has been read and compressed. On the other hand, the decompression will start on the server only when all the data has been received. A better solution to achieve the same result involves the use of streams. On the client machine, streams allows you to compress and send the data chunks as soon as they are read from the filesystem, whereas, on the server, it allows you to decompress every chunk as soon as it is received from the remote peer."\cite{nodejsbook}

%\paragraph{Composability.}
%"The code we have seen so far has already given us an overview of how streams can be composed, thanks to the pipe() method, which allows us to connect the different processing units, each being responsible for one single functionality in perfect Node.js style. 
%This is possible because streams have a uniform interface, and they can understand each other in terms of API. The only prerequisite is that the next stream in the pipeline has to support the data type produced by the previous stream, which can be either binary, text, or even objects, as we will see later in the chapter.\\
%For these reasons, streams are often used not just to deal with pure I/O, but also as a means to simplify and modularize the code."\cite{nodejsbook}

\subsection{Anatomy of Streams}
"Every stream in Node.js is an implementation of one of the four base abstract classes available in the stream core module:

\begin{itemize}
\item stream.Readable
\item stream.Writable
\item stream.Duplex
\item stream.Transform
\end{itemize}

Each stream class is also an instance of EventEmitter. 
Streams, in fact, can produce several types of events, such as end, when a Readable stream has finished reading, or error, when something goes wrong.

One of the reasons why streams are so flexible is the fact that they can handle not only binary data, but practically, almost any JavaScript value; in fact they can support two operating modes:
\begin{itemize}
\item Binary mode: This mode is where data is streamed in the form of chunks, such as buffers or strings;
\item Object mode: This mode is where the streaming data is treated as a sequence of discreet objects (allowing to use almost any JavaScript value).
\end{itemize}

\paragraph{Readable streams.}
A readable stream represents a source of data; in Node.js, it's implemented using the Readable abstract class that is available in the stream module.

\paragraph{Writable streams.}
A writ[e]able stream represents a data destination; in Node.js, it's implemented using the Writ[e]able abstract class, which is available in the stream module.

\paragraph{Duplex streams.}
A Duplex stream is a stream that is both Readable and Writ[e]able.
It is useful when we want to describe an entity that is both a data source and a data destination, as for example, network sockets. 
Duplex streams inherit the methods of both stream.Readable and stream.Writable, so this is nothing new to us. 
This means that we can read() or write() data, or listen for both the readable and drain events.

\paragraph{Transform streams.}
The Transform streams are a special kind of Duplex stream that are specifically designed to handle data transformations.
In a simple Duplex stream, there is no immediate relationship between the data read from the stream and the data written into it (at least, the stream is agnostic to such a relationship).
On the other side, Transform streams apply some kind of transformation to each chunk of data that they receive from their Writable side and then make the transformed data available on their Readable side.
From the outside, the interface of a Transform stream is exactly like that of a Duplex stream.
However, when we want to build a new Duplex stream we have to provide the read() and write() methods while, for implementing a new Transform stream, we have to fill in another pair of methods: transform() and flush()."\cite{nodejsbook}


\subsection{Piping patterns}
"As in real-life plumbing, Node.js streams also can be piped together following different patterns; we can, in fact, merge the flow of two different streams into one, split the flow of one stream into two or more pipes, or redirect the flow based on a condition. 
In this section, we are going to explore the most important plumbing techniques that can be applied to Node.js streams."\cite{nodejsbook}
\begin{itemize}
	\item Combining streams - encapsulation of sequentualy connected streams in to single looking stream with single I/O points and single error handling mechanism by pipeing readable stream in to writable stream.\cite{nodejsbook}
	\item Forking streams - piping single readable in to multiple writable streams.\cite{nodejsbook}
	\item Merging streams - piping multiple readable streams in to single writable stream.\cite{nodejsbook}
	\item Multiplexing and demultiplexing - forking and merging pattern which provides shared communication chanel for entities from different sreams.\cite{nodejsbook}
\end{itemize}
