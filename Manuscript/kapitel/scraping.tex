\chapter{Interaction with banks}
	\section{Scraping}
	"Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites. Usually, such software programs simulate human exploration of the World Wide Web by either implementing low-level Hypertext Transfer Protocol (HTTP), or embedding a fully-fledged web browser, such as Mozilla Firefox.\\
	Web scraping is the process of automatically collecting information from the World Wide Web. It is a field with active developments sharing a common goal with the semantic web vision, an ambitious initiative that still requires breakthroughs in text processing, semantic understanding, artificial intelligence and human-computer interactions. Current web scraping solutions range from the ad-hoc, requiring human effort, to fully automated systems that are able to convert entire web sites into structured information, with limitations." \cite{wikiScraping}
	\section{DOM and its parsing}
	"The Document Object Model is a platform- and language-neutral interface that will allow programs and scripts to dynamically access and update the content, structure and style of documents." \cite{w3cDOM}
	\section{CasperJS}
		\cite{casperjs}
		CasperJS is an open source navigation scripting \& testing utility written in Javascript for the PhantomJS WebKit headless browser and SlimerJS (Gecko). It eases the process of defining a full navigation scenario and provides useful high-level functions, methods \& syntactic sugar for doing common tasks such as:
\begin{itemize}
	\item defining \& ordering browsing navigation steps
	\item filling \& submitting forms
	\item clicking \& following links
	\item capturing screenshots of a page (or part of it)
	\item testing remote DOM
	\item logging events
	\item downloading resources, including binary ones
	\item writing functional test suites, saving results as JUnit XML
	\item scraping Web contents
\end{itemize}

	